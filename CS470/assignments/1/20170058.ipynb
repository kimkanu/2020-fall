{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"assignment1.ipynb","provenance":[{"file_id":"1Z_KFYyPQoW9AEQE0tz9k-6xSv54mguOg","timestamp":1600350787665},{"file_id":"16hRrCgP0C4Vngivv5R2KaxYD7ivJp6G0","timestamp":1600301416878}],"collapsed_sections":["I3NY7sfLled4"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WTUYbQLlYUQ-","colab_type":"text"},"source":["# CS470 Assignment #1: Image Classification using Convolutional Neural Networks (CNNs) \n","---\n","TA : Whie Jung (whieya@kaist.ac.kr), Sunghyun Myung (shm_@kaist.ac.kr), Hajin Shim (shimazing@kaist.ac.kr)\n","\n","---\n","\n","## Instructions\n","- In this assignment, we will classify the images in CIFAR10 dataset into 10 categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) using Convolutional Neural Networks(CNNs).  \n","\n","- To this end, you need to implement necessary network components (e.g. residual blocks) using nn.Module class and complete whole CNNs with those blocks. Then, you will experiment those network architectures using given train/testing pipeline and report classfication accuracies on the test set.      \n","\n","- In each part, you will be given a starter code for the implementation. Please read the attached illustrations and instructions carefully to implement the codes.  \n","\n","- As you follow the given steps, fill in the section marked ***Px.x*** (e.g. P1.1, P1.2, etc) with the appropriate code. **Note that you can only fill those marked areas, and cannot modify rest of the  skeleton code.**  \n","\n","- In short, you should (1) complete the code, (2) experiment with several configurations of CNNs, and (3) report the final classification accuracies on the CIFAR10 test set.\n","- To start with, you should download this ipynb file into your own google drive.\n","You can save the file into your own google drive by clicking `make a copy(사본만들기)`. Find the copy in your drive, change their name to `assignment1.ipynb`, if their names were changed to e.g. `Copy of assignment1.ipyb` or `assignment1.ipynb의 사본`. \n","\n","## Submission guidelines\n","- Your code and report will be all in Colab. \n","- <font color=\"red\"> You will get the full credit **only if** you complete the code **and** match your results(classifiction accuarcy, number of parameter) with the values we provided on the bottom of the project. Once you trained all the models, we will automatically collect the experimental results.\n"," </font>\n","- You may download this notebook to run the code on a local machine. However, we should be able to reproduce your results on Colab using your code. Please double-check if your code runs without error and reproduces your results **on Colab**. Submissions failed to run or reproduce the results will get a substantial penalty. \n","\n","## Deliverables\n","- Download the following files from your google drive, and submit them in a zip file named as **[StudentID].zip**. For example, if your student ID is 20201234, the file name should be **20201234.zip**.\n","  - **[StudentID].ipynb**: Your Colab notebook. <font color=\"red\"> As a proof that you've ran this code by yourself, make sure your notebook contains the output of each code block. Note that the score of the section 3. will be evaluated by the final output of the last code. </font>\n","  - **[StudentID]_[model].pt**: the model checkpoints. You have to submit **4** models in total: `conv_best.pt`, `resPlain_best.pt`, `resBottleneck_best.pt`, and `inception_best.pt`.\n","- Your assignment should be submitted through **KLMS**. All other submissions (e.g., via email) will not be considered as valid submissions. \n","\n","## Due date\n","- **23:59:59 September 23rd.** \n","- Late submission is allowed until 23:59:59 September 25th.\n","- Late submissions will have 20% penalty.\n","\n","## <font color=\"red\"> Caution </font>\n","- <font color=\"red\"> Due to GPU resource restrictions on Google Colab, you may not be able to use Colab GPU near the due date. Therefore, **we strongly advise you to start every assignment early.** </font>\n","- <font color=\"red\"> Again, do not modify the skeleton codes. Only write your code inside the designated area. </font>\n","\n","## Questions\n","- Please use QnA board in KLMS as a main communication channel. When you post questions, please make it public so that all students can share the information. Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n","\n","## Changelog\n","09/17 02:00: Illustrations for `InceptionBlock` and `ResBlockBottleneck` have been updated.  \n","09/17 02:10: We fixed the code counting the parameters of each model in \"Train Models Through the Pipeline\".\n"]},{"cell_type":"markdown","metadata":{"id":"I3NY7sfLled4","colab_type":"text"},"source":["---\n","\n","---\n","\n","\n","# Prerequisite: change the runtime type to **GPU**.\n","\n","![test](https://docs.google.com/uc?export=download&id=1Jugrjl86L9EY1ePTjH8OVMFq7gmZsoz_)"]},{"cell_type":"markdown","metadata":{"id":"GYfj23oOmOr6","colab_type":"text"},"source":["---\n","# Prerequisite: mount your gdrive."]},{"cell_type":"code","metadata":{"id":"FQdqM8z8ZM6l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"status":"ok","timestamp":1600389715069,"user_tz":-540,"elapsed":691,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"fd2ab028-db2b-456d-e4d8-f66daef8a717"},"source":["# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n","# login with your google account and type authorization code to mount on your google drive.\n","import os\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0W1NJ35eNLmt","colab_type":"text"},"source":["---\n","# Prerequisite: setup the `root` directory properly."]},{"cell_type":"code","metadata":{"id":"NlhMEGFUi-0c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389719531,"user_tz":-540,"elapsed":753,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["# Specify the directory path where `assignemnt1.ipynb` exists.\n","# For example, if you saved `assignment1.ipynb` in `/gdrive/My Drive/cs470/assignment1` directory,\n","# then set root = '/gdrive/My Drive/cs470/assignment1'\n","root = '/gdrive/My Drive/CS470/assignment1'"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8ANEG5WmXzS","colab_type":"text"},"source":["---\n","# Import libraries"]},{"cell_type":"code","metadata":{"id":"oxNKZxIRYURA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389722620,"user_tz":-540,"elapsed":1042,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["from PIL import Image\n","from tqdm import tqdm\n","from pathlib import Path\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.datasets import CIFAR10\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTx3YvqWYURE","colab_type":"text"},"source":["-----\n","\n","# 1.Implementing Network Modules\n","\n","In this assignment, you will implement four modularized blocks and one network class as follows:\n","\n","**Block classes**  \n","(Example) Multilayer perceptron Block (MLPBlock) **To provide a starting point, the solutions for this section are given below.**  \n","(1) Convolutional block (ConvBlock)   \n","(2) Plain residual block (ResBlockPlain)  \n","(3) Residual block with bottleneck (ResBlockBottleneck)  \n","(4) Inception Block (InceptionBlock)\n","\n","**Network class**  \n","(1) MyNetwork \n","\n","In each cell, there is a starter code, a schematic illustration, and instructions that will guide you to implement each module correctly. Specifically, the schematic illustrations are to show you the computational graphs of modules, which give you high-level views on how the modules should be constructed and work. (E.g. which nn.Module to use, or input/output shape of each layer written in italics). Therefore, please read the illustrations and instructions carefully to complete the codes.\n","<!-- \n","Below is an example.\n","\n","### Example: ConvLayer Module [(Illustration)](https://docs.google.com/drawings/d/1_aPhPSPgh5-5FEfI_jnfp8r6-wNjY_QYXBT3zzjkHk0/edit?usp=sharing) -->"]},{"cell_type":"markdown","metadata":{"id":"0o6mOW0_cjaR","colab_type":"text"},"source":["## Block class"]},{"cell_type":"markdown","metadata":{"id":"PILjK1Nmx2M6","colab_type":"text"},"source":["### (Example) Implement MLP Block [(Illustration)](https://docs.google.com/drawings/d/1gTPLeK0H5ooMcn7CNPysqwr9_07fTqkHE4-T3ZqyhPo/edit?usp=sharing)  "]},{"cell_type":"code","metadata":{"id":"Yo9_pniOx1kd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389725198,"user_tz":-540,"elapsed":792,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class MLPBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(MLPBlock, self).__init__()\n","        \"\"\"\n","        Initialize a basic multi-layer perceptron module components.\n","        Illustration: https://docs.google.com/drawings/d/1gTPLeK0H5ooMcn7CNPysqwr9_07fTqkHE4-T3ZqyhPo/edit?usp=sharing\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","        \n","        Args:\n","            1. in_channels (int): Number of channels in input.\n","            2. out_channels (int): Number of channels to be produced.\n","        \"\"\"\n","        #######################################\n","        ## This section is an example.       ##        \n","        self.fc1 = nn.Linear(in_channels, 512)\n","        self.bn1 = nn.BatchNorm1d(512)\n","        self.fc2 = nn.Linear(512,128)\n","        self.bn2 = nn.BatchNorm1d(128)\n","        self.fc3 = nn.Linear(128, out_channels)\n","        self.bn3 = nn.BatchNorm1d(out_channels)\n","        self.act = nn.ReLU()\n","        #######################################\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Feed-forward data 'x' through the module.\n","                \n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized components in __init__ method.\n","\n","        Args:\n","            1. x (torch.FloatTensor): A tensor of shape (B, in_channels)\n","            .\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels). \n","        \"\"\"\n","        #######################################\n","        ## This section is an example.       ##\n","        output = self.act(self.bn1(self.fc1(x)))\n","        output = self.act(self.bn2(self.fc2(output)))\n","        output = self.act(self.bn3(self.fc3(output)))\n","        #######################################\n","        return output"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eLCjNVBx2sd","colab_type":"text"},"source":["### (1) Implement Convolutional Block[(Illustration)](https://docs.google.com/drawings/d/1MRYBywpuazlldwC11UTa-kuWMWEDsFewDnirKiFX5us/edit?usp=sharing) (10pt) "]},{"cell_type":"code","metadata":{"id":"UaRZybdRx01E","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389726228,"user_tz":-540,"elapsed":457,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, \n","                 padding=1):\n","        super(ConvBlock, self).__init__()\n","        \"\"\"\n","        Initialize a basic convolutional layer module components.\n","        Illustration: https://docs.google.com/drawings/d/1MRYBywpuazlldwC11UTa-kuWMWEDsFewDnirKiFX5us/edit?usp=sharing\n","\n","        Args:\n","            1. in_channels (int): Number of channels in the input. \n","            2. out_channels (int): Number of channels produced.\n","            3. kernel_size (int) : Size of the kernel used in conv layer (Default:3)\n","            4. stride (int) : Stride of the convolution (Default:1)\n","            5. padding (int) : Zero-padding added to both sides of the input (Default:1)\n","        \"\"\"\n","        #################################\n","        ## P1.1. Write your code here  ##\n","        self.cv = nn.Conv2d(\n","            in_channels=in_channels,\n","            out_channels=out_channels,\n","            kernel_size=kernel_size,\n","            stride=stride,\n","            padding=padding,\n","            bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.act = nn.ReLU()\n","        #################################\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Feed-forward the data 'x' through the module.\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized components in __init__ method.\n","\n","        Args:\n","            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n","            \n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W). \n","        \"\"\"\n","        #################################\n","        ## P1.2. Write your code here  ##     \n","        output = self.act(self.bn(self.cv(x)))\n","        #################################\n","        return output"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fjz2alvvyr6T","colab_type":"text"},"source":["### (2) Implement ResBlockPlain [(Illustration)](https://docs.google.com/drawings/d/19FS5w7anbTAF6UrMPdM4fs8nk9x3Lm5KRIODawC4duQ/edit?usp=sharing) (10pt)"]},{"cell_type":"code","metadata":{"id":"uCeMmP-eyqj-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389727203,"user_tz":-540,"elapsed":814,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class ResBlockPlain(nn.Module):\n","    def __init__(self, in_channels):\n","        super(ResBlockPlain, self).__init__()\n","        \"\"\"Initialize a residual block module components.\n","\n","        Illustration: https://docs.google.com/drawings/d/19FS5w7anbTAF6UrMPdM4fs8nk9x3Lm5KRIODawC4duQ/edit?usp=sharing\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","\n","        Args:\n","            1. in_channels (int): Number of channels in the input.\n","        \"\"\"\n","        #################################\n","        ## P2.1. Write your code here ##\n","        self.cv1 = nn.Conv2d(\n","            in_channels,\n","            in_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn1 = nn.BatchNorm2d(in_channels)\n","        self.cv2 = nn.Conv2d(\n","            in_channels,\n","            in_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn2 = nn.BatchNorm2d(in_channels)\n","        self.act = nn.ReLU()\n","        #################################\n","\n","    def forward(self, x):\n","        \"\"\"Feed-forward the data `x` through the network.\n","\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized components in __init__ method.\n","\n","        Args:\n","            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n","\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W). \n","        \"\"\"\n","        ################################\n","        ## P2.2. Write your code here ## \n","        output = self.act(self.bn1(self.cv1(x)))\n","        output = self.bn2(self.cv2(output))\n","        output = self.act(torch.add(x, output))\n","        ################################\n","        return output "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0yAzCxaysW0","colab_type":"text"},"source":["### (3) Implement ResBlockBottleneck [(Illustration)](https://docs.google.com/drawings/d/1n2E0TwiWhf1IGdD16-MeQjzUcys_V7ETTzn33j_bEy0/edit?usp=sharing) (10pt)  "]},{"cell_type":"code","metadata":{"id":"RDNU2zPTy_8n","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389727560,"user_tz":-540,"elapsed":653,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class ResBlockBottleneck(nn.Module):\n","    def __init__(self, in_channels, hidden_channels):\n","        super(ResBlockBottleneck, self).__init__()\n","        \"\"\"Initialize a residual block module components.\n","\n","        Illustration: https://docs.google.com/drawings/d/1n2E0TwiWhf1IGdD16-MeQjzUcys_V7ETTzn33j_bEy0/edit?usp=sharing\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","\n","        Args:\n","            1. in_channels (int): Number of channels in the input. \n","            2. hidden_channels (int): Number of hidden channels produced by the first ConvLayer module.\n","        \"\"\"\n","        #################################\n","        ## P3.1. Write your code here  ##\n","        self.cv1 = nn.Conv2d(\n","            in_channels,\n","            hidden_channels,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False)\n","        self.bn1 = nn.BatchNorm2d(hidden_channels)\n","        self.cv2 = nn.Conv2d(\n","            hidden_channels,\n","            hidden_channels,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn2 = nn.BatchNorm2d(hidden_channels)\n","        self.cv3 = nn.Conv2d(\n","            hidden_channels,\n","            in_channels,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False)\n","        self.bn3 = nn.BatchNorm2d(in_channels)\n","\n","        self.act = nn.ReLU()\n","        #################################\n","\n","    def forward(self, x):\n","        \"\"\"Feed-forward the data `x` through the network.\n","\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized components in __init__ method.\n","\n","        Args:\n","            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n","\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W). \n","        \"\"\"\n","        ################################\n","        ## P3.2. Write your code here ##\n","        output = nn.Sequential(\n","            self.cv1,\n","            self.bn1,\n","            self.act,\n","            self.cv2,\n","            self.bn2,\n","            self.act,\n","            self.cv3,\n","            self.bn3\n","        )(x)\n","        output = self.act(torch.add(x, output))\n","        ################################\n","        return output"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkXfj1KsytsU","colab_type":"text"},"source":["### (4) Implement InceptionBlock[(Illustration)](https://docs.google.com/drawings/d/1I020R1YqVAr8LWKHgm7N5J5fzFpHvx1fqXuAs6z8qyE/edit?usp=sharing)  (20pt)"]},{"cell_type":"code","metadata":{"id":"-nLdTePuyrCX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389728589,"user_tz":-540,"elapsed":1117,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class InceptionBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(InceptionBlock, self).__init__()\n","        \"\"\"Initialize a basic InpcetionBlock module components.\n","\n","        Illustration: https://docs.google.com/drawings/d/1I020R1YqVAr8LWKHgm7N5J5fzFpHvx1fqXuAs6z8qyE/edit?usp=sharing\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","\n","        Args:\n","            1. in_channels (int): Number of channels in the input. \n","            2. out_channels (int): Number of channels in the final output.\n","        \"\"\"\n","        assert out_channels%8==0, 'out channel should be mutiplier of 8'\n","\n","        ################################\n","        ## P4.1. Write your code here ##\n","\n","        # Conv 1x1 branch\n","        self.one_cv = nn.Conv2d(\n","            in_channels,\n","            out_channels//4,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False\n","        )\n","        self.one_bn = nn.BatchNorm2d(out_channels//4)\n","\n","        # Conv 3x3 branch\n","        self.three_cv1 = nn.Conv2d(\n","            in_channels,\n","            out_channels//2,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False\n","        )\n","        self.three_bn1 = nn.BatchNorm2d(out_channels//2)\n","        self.three_cv2 = nn.Conv2d(\n","            out_channels//2,\n","            out_channels//2,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False\n","        )\n","        self.three_bn2 = nn.BatchNorm2d(out_channels//2)\n","\n","        # Conv 5x5 branch\n","        self.five_cv1 = nn.Conv2d(\n","            in_channels,\n","            out_channels//8,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False\n","        )\n","        self.five_bn1 = nn.BatchNorm2d(out_channels//8)\n","        self.five_cv2 = nn.Conv2d(\n","            out_channels//8,\n","            out_channels//8,\n","            kernel_size=5,\n","            stride=1,\n","            padding=2,\n","            bias=False\n","        )\n","        self.five_bn2 = nn.BatchNorm2d(out_channels//8)\n","\n","        # Maxpool branch\n","        self.mp_mp = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n","        self.mp_cv = nn.Conv2d(\n","            in_channels,\n","            out_channels//8,\n","            kernel_size=1,\n","            stride=1,\n","            padding=0,\n","            bias=False\n","        )\n","        self.mp_bn = nn.BatchNorm2d(out_channels//8)\n","\n","        self.act = nn.ReLU()\n","\n","        ################################\n","\n","    def forward(self, x):\n","        \"\"\"Feed-forward the data `x` through the module.\n","\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized components in the __init__ method.\n","\n","        Args:\n","            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n","\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W). \n","\n","        \"\"\"\n","        ################################\n","        ## P4.2. Write your code here ##\n"," \n","        # conv 1x1\n","        one_output = nn.Sequential(\n","            self.one_cv,\n","            self.one_bn,\n","            self.act\n","        )(x)\n","\n","        # conv 3x3\n","        three_output = nn.Sequential(\n","            self.three_cv1,\n","            self.three_bn1,\n","            self.act,\n","            self.three_cv2,\n","            self.three_bn2,\n","            self.act,\n","        )(x)\n","\n","        # conv 5x5\n","        five_output = nn.Sequential(\n","            self.five_cv1,\n","            self.five_bn1,\n","            self.act,\n","            self.five_cv2,\n","            self.five_bn2,\n","            self.act,\n","        )(x)\n","\n","        # maxpool\n","        mp_output = nn.Sequential(\n","            self.mp_mp,\n","            self.mp_cv,\n","            self.mp_bn,\n","            self.act,\n","        )(x)\n","\n","        output = torch.cat((\n","          one_output,\n","          three_output,\n","          five_output,\n","          mp_output\n","        ), 1)\n"," \n","        ################################\n","        return output"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5irFIyBYc2nM","colab_type":"text"},"source":["## Network class"]},{"cell_type":"markdown","metadata":{"id":"yXSb4yHIYURW","colab_type":"text"},"source":["### (Example) MyNetworkExample\n","\n","The class `MyNetworkExample` is a sample network using `MLPBlock` implemented above. **You don't have to implement anything in this code section.**"]},{"cell_type":"code","metadata":{"id":"0EVcyU6UE_N3","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389731039,"user_tz":-540,"elapsed":700,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class MyNetworkExample(nn.Module):\n","    def __init__(self, nf, block_type='mlp'):\n","        super(MyNetworkExample, self).__init__()\n","        \"\"\"Initialize an entire network module components.\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","\n","        Args:        \n","            1. nf (int): Number of input channels for the first nn.Linear Module. An abbreviation for num_filter.\n","            2. block_type (str, optional): Type of blocks to use. ('mlp'. default: 'mlp')\n","        \"\"\"\n","        #######################################\n","        ## This section is an example.       ##\n","        if block_type == 'mlp':\n","            block = MLPBlock\n","            # Since shape of input image is 3 x 32 x 32, the size of flattened input is 3*32*32. \n","            self.mlp = block(3*32*32, nf)\n","            self.fc = nn.Linear(nf, 10)\n","        else:\n","            raise Exception(f\"Wrong type of block: {block_type}.Expected : mlp\")\n","        #######################################\n","\n","    def forward(self, x):\n","        \"\"\"Feed-forward the data `x` through the network.\n","\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized network components in __init__ method.\n","        Args:\n","            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n","\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, 10). \n","        \"\"\"\n","        #######################################\n","        ## This section is an example.       ##\n","        output = self.mlp(x.view(x.size()[0], -1))\n","        output = self.fc(output)\n","        return output\n","        #######################################"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbSoxtKYU_gG","colab_type":"text"},"source":["### (1) MyNetwork[(Illustration)](https://docs.google.com/drawings/d/1L8PYO8A1EL4BN4bzTWH4ygr-WiS7NDeFz7P1PkhBZwE/edit?usp=sharing) (10pt)\n","\n","There are two functions to implement in this section. **Read the comments and illustration carefully before you type anything.**"]},{"cell_type":"code","metadata":{"id":"UvdXrQUqYURW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600389732110,"user_tz":-540,"elapsed":827,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["class MyNetwork(nn.Module):\n","    def __init__(self, nf, block_type='conv', num_blocks=[1, 1, 1]):\n","        super(MyNetwork, self).__init__()\n","        \"\"\"Initialize an entire network module components.\n","\n","        Illustration: https://docs.google.com/drawings/d/1L8PYO8A1EL4BN4bzTWH4ygr-WiS7NDeFz7P1PkhBZwE/edit?usp=sharing\n","\n","        Instructions:\n","            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n","            2. Initialized network components will be referred in `forward` method \n","               for constructing the dynamic computational graph.\n","\n","        Args:\n","            1. nf (int): Number of output channels for the first nn.Conv2d Module. An abbreviation for num_filter.\n","            2. block_type (str, optional): Type of blocks to use. ('conv' | 'resPlain' | 'resBottleneck' | 'inception'. default: 'conv')\n","            3. num_blocks (list or tuple, optional): A list or tuple of length 3. \n","               Each item at i-th index indicates the number of blocks at i-th Layer.  \n","               (default: [1, 1, 1])\n","        \"\"\"\n","        \n","        self.block_type = block_type\n","\n","        # Define blocks according to block_type\n","        if self.block_type == 'conv':\n","            block = ConvBlock\n","            block_args = lambda x: (x, x, 3, 1, 1)\n","        elif self.block_type == 'resPlain':\n","            block = ResBlockPlain\n","            block_args = lambda x: (x,)\n","        elif self.block_type == 'resBottleneck':\n","            block = ResBlockBottleneck\n","            block_args = lambda x: (x, x//2)\n","        elif self.block_type == 'inception':\n","            block = InceptionBlock\n","            block_args = lambda x: (x, x)\n","        else:\n","            raise Exception(f\"Wrong type of block: {block_type}\")\n","\n","        # Define block layer by stacking multiple blocks. \n","        # You don't need to modify it. Just use these block layers in forward function.  \n","        self.block1 = nn.Sequential(*[block(*block_args(nf)) for _ in range(num_blocks[0])])\n","        self.block2 = nn.Sequential(*[block(*block_args(nf*2)) for _ in range(num_blocks[1])])\n","        self.block3 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[2])])\n","\n","        ################################\n","        ## P5.1. Write your code here ##\n","        self.cv1 = nn.Conv2d(\n","            3,\n","            nf*1,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn1 = nn.BatchNorm2d(nf)\n","        self.cv2 = nn.Conv2d(\n","            nf*1,\n","            nf*2,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn2 = nn.BatchNorm2d(nf*2)\n","        self.cv3 = nn.Conv2d(\n","            nf*2,\n","            nf*4,\n","            kernel_size=3,\n","            stride=1,\n","            padding=1,\n","            bias=False)\n","        self.bn3 = nn.BatchNorm2d(nf*4)\n","        self.fc = nn.Linear(nf*4, 10)\n","\n","        self.act = nn.ReLU()\n","        self.mp = nn.MaxPool2d(2, stride=2)\n","        self.aap = nn.AdaptiveAvgPool2d((1,1))\n","        ################################\n","\n","    def forward(self, x):\n","        \"\"\"Feed-forward the data `x` through the network.\n","\n","        Instructions:\n","            1. Construct the feed-forward computational graph as illustrated in the link \n","               using the initialized network components in __init__ method.\n","        Args:\n","            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n","\n","        Returns:\n","            1. output (torch.FloatTensor): An output tensor of shape (B, 10). \n","        \"\"\"\n","\n","        #######################################################################\n","        ## P5.2. Write your code here                                        ##\n","        ## Hint : use self.block1, self.block2, self.block3 for block layers ##\n","        output = nn.Sequential(\n","            self.cv1,\n","            self.bn1,\n","            self.act,\n","            self.mp,\n","            self.block1,\n","            self.cv2,\n","            self.bn2,\n","            self.act,\n","            self.mp,\n","            self.block2,\n","            self.cv3,\n","            self.bn3,\n","            self.act,\n","            self.mp,\n","            self.block3,\n","            self.aap,\n","            nn.Flatten(),\n","            self.fc\n","        )(x)\n","        #######################################################################\n","        return output"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_kBr92qYURi","colab_type":"text"},"source":["---\n","\n","# 2.Experiment with Train/Test Pipeline\n","\n","This section contains the entire train and test loop of the pipeline, specifically the followings:\n","1. feed inputs into the network, get outputs, and then compute classification loss. \n","2. backward the computed loss and update network weights (only in the training loop).\n","3. save tensorboard logs frequently.\n","4. save checkpoint weights frequently.\n","\n","**There are no modifications necessary in this section.** Run the code and enjoy!"]},{"cell_type":"markdown","metadata":{"id":"ZcFgC8fd4gsr","colab_type":"text"},"source":["## Arguments and Environment Settings\n","\n","This section contains code that\n","- defines miscellaneous arguments for our pipeline.\n","- runs Tensorboard to visualize accuracy and loss curves.\n","\n","Optionally, you may change `args.ckpt_iter` and `args.`log_iter` as you wish to save space in your Google Drive.\n","\n"]},{"cell_type":"code","metadata":{"id":"0vcdKxr7YURj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600391222393,"user_tz":-540,"elapsed":785,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["# Configurations & Hyper-parameters\n","\n","from easydict import EasyDict as edict\n","\n","# set manual seeds \n","torch.manual_seed(470)\n","torch.cuda.manual_seed(470)\n","\n","args = edict()\n","\n","# basic options \n","args.name = 'main'                   # experiment name.\n","args.ckpt_dir = 'ckpts'              # checkpoint directory name.\n","args.ckpt_iter = 1000                # how frequently checkpoints are saved.\n","args.ckpt_reload = 'best'            # which checkpoint to re-load.\n","args.gpu = True                      # whether or not to use gpu. \n","\n","# network options\n","args.num_filters = 16                # number of output channels in the first nn.Conv2d module in MyNetwork.\n","args.block_type = 'mlp'              # type of block. ('mlp' | 'conv' | 'resPlain' | 'resBottleneck' | 'inception').\n","args.num_blocks = [5, 5, 5]          # number of blocks in each Layer.\n","\n","# data options\n","args.dataroot = 'dataset/cifar10'    # where CIFAR10 images exist.\n","args.batch_size = 128                # number of mini-batch size.\n","\n","# training options\n","args.lr = 0.1                        # learning rate.\n","args.epoch = 100                     # training epoch.\n","\n","# tensorboard options\n","args.tensorboard = True              # whether or not to use tensorboard logging.\n","args.log_dir = 'logs'                # to which tensorboard logs will be saved.\n","args.log_iter = 100                  # how frequently logs are saved."],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-acigO4YURl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600391224961,"user_tz":-540,"elapsed":733,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["# Basic settings\n","device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n","\n","result_dir = Path(root) / 'results'\n","result_dir.mkdir(parents=True, exist_ok=True)\n","\n","global_step = 0\n","best_accuracy = 0."],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOTK_7HjYURv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":56},"executionInfo":{"status":"ok","timestamp":1600391227685,"user_tz":-540,"elapsed":2579,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"3882eb64-782a-49f0-a969-6dbe9fa13881"},"source":["# Define train/test data loaders  \n","# Use data augmentation in training set to mitigate overfitting. \n","train_transform = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),                                \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","    ])\n","\n","test_transform = transforms.Compose([                       \n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n","    ])\n","\n","train_dataset = CIFAR10(args.dataroot, download=True, train=True, transform=train_transform)\n","test_dataset = CIFAR10(args.dataroot, download=True, train=False, transform=test_transform)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KqZmn8qQdBji","colab_type":"text"},"source":["## Tracking the states with Tensorboard\n","\n","In following training stage, losses and accuracies will be logged on the tensorboard. It provides an useful data for analyzing training process. \n","Use tensorboard wisely.\n","\n"]},{"cell_type":"code","metadata":{"id":"t0Ih-Yg0YURn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":898},"executionInfo":{"status":"ok","timestamp":1600391228655,"user_tz":-540,"elapsed":722,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"e64eba4c-69eb-4796-c7fb-96c272c55ae4"},"source":["# Setup tensorboard.\n","if args.tensorboard:\n","    from torch.utils.tensorboard import SummaryWriter \n","    %load_ext tensorboard\n","    %tensorboard --logdir \"/gdrive/My Drive/{str(result_dir).replace('/gdrive/My Drive/', '')}\"\n","else:\n","    writer = None"],"execution_count":27,"outputs":[{"output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["Reusing TensorBoard on port 6006 (pid 305), started 1:56:14 ago. (Use '!kill 305' to kill it.)"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["\n","        (async () => {\n","            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n","            const iframe = document.createElement('iframe');\n","            iframe.src = url;\n","            iframe.setAttribute('width', '100%');\n","            iframe.setAttribute('height', '800');\n","            iframe.setAttribute('frameborder', 0);\n","            document.body.appendChild(iframe);\n","        })();\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ZmrnUO_55yix","colab_type":"text"},"source":["## The Train-and-Test pipeline"]},{"cell_type":"code","metadata":{"id":"h-ukFLINYURx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600391230933,"user_tz":-540,"elapsed":755,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["def train_net(net, optimizer, scheduler, block_type, writer):\n","    global_step = 0\n","    best_accuracy = 0\n","\n","    for epoch in range(args.epoch):\n","        # Here starts the train loop.\n","        net.train()\n","        for batch_idx, (x, y) in enumerate(train_dataloader):\n","\n","            global_step += 1\n","\n","            #  Send `x` and `y` to either cpu or gpu using `device` variable. \n","            x = x.to(device=device)\n","            y = y.to(device=device)\n","            \n","            # Feed `x` into the network, get an output, and keep it in a variable called `logit`. \n","            logit = net(x)\n","\n","            # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n","            accuracy = (logit.argmax(1) == y).float().mean()\n","\n","            # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n","            loss = nn.CrossEntropyLoss()(logit, y)\n","\n","            # flush out the previously computed gradient.\n","            optimizer.zero_grad()\n","\n","            # backward the computed loss. \n","            loss.backward()\n","\n","            # update the network weights. \n","            optimizer.step()\n","\n","            if global_step % args.log_iter == 0 and writer is not None:\n","                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log. \n","                writer.add_scalar('train_loss', loss, global_step)\n","                writer.add_scalar('train_accuracy', accuracy, global_step)\n","\n","            if global_step % args.ckpt_iter == 0: \n","                # Save network weights in the directory specified by `ckpt_dir` directory. \n","                torch.save(net.state_dict(), f'{ckpt_dir}/{global_step}.pt')\n","\n","        # Here starts the test loop.\n","        net.eval()\n","        with torch.no_grad():\n","            test_loss = 0.\n","            test_accuracy = 0.\n","            test_num_data = 0.\n","            for batch_idx, (x, y) in enumerate(test_dataloader):\n","                # Send `x` and `y` to either cpu or gpu using `device` variable..\n","                x = x.to(device=device)\n","                y = y.to(device=device)\n","\n","                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n","                logit = net(x)\n","\n","                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n","                loss = nn.CrossEntropyLoss()(logit, y)\n","\n","                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n","                accuracy = (logit.argmax(dim=1) == y).float().mean()\n","\n","                test_loss += loss.item()*x.shape[0]\n","                test_accuracy += accuracy.item()*x.shape[0]\n","                test_num_data += x.shape[0]\n","\n","            test_loss /= test_num_data\n","            test_accuracy /= test_num_data\n","\n","            if writer is not None: \n","                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log. \n","                writer.add_scalar('test_loss', test_loss, global_step)\n","                writer.add_scalar('test_accuracy', test_accuracy, global_step)\n","\n","                # Just for checking progress\n","                print(f'Test result of epoch {epoch}/{args.epoch} || loss : {test_loss:.3f} acc : {test_accuracy:.3f} ')\n","\n","                writer.flush()\n","\n","            # Whenever `test_accuracy` is greater than `best_accuracy`, save network weights with the filename 'best.pt' in the directory specified by `ckpt_dir`.\n","            if test_accuracy > best_accuracy:\n","                best_accuracy = test_accuracy\n","                torch.save(net.state_dict(), f'{ckpt_dir}/{block_type}_best.pt')\n","    \n","        scheduler.step()\n","    return best_accuracy\n"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbg5zK-d58r6","colab_type":"text"},"source":["## Train Models Through the Pipeline\n","\n","Training a single model for 100 epochs will take around 40~50 minutes. Use this information as an indicator for your experiments.\n","\n"]},{"cell_type":"code","metadata":{"id":"z9amkLXJKvPN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600391232950,"user_tz":-540,"elapsed":706,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["# Function for weight initialization.\n","def weight_init(m):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        torch.nn.init.kaiming_normal_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","    elif isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.constant_(m.weight, 1)\n","        torch.nn.init.constant_(m.bias, 0)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"rf7OLqg7Fy1T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"ok","timestamp":1600391233663,"user_tz":-540,"elapsed":882,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"8a8bc2fc-7e48-4860-a345-feacddfc71fc"},"source":["# List of all block types we will use.\n","block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n","\n","# Create directory name.\n","num_trial=0\n","parent_dir = result_dir / f'trial_{num_trial}'\n","while parent_dir.is_dir():\n","    num_trial = int(parent_dir.name.replace('trial_',''))\n","    parent_dir = result_dir / f'trial_{num_trial+1}'\n","print(f'Logs and ckpts will be saved in : {parent_dir}')\n","\n","# Define networks\n","networks = []\n","for block_type in block_types:\n","    if block_type == 'conv':\n","        args.num_blocks = [10, 10, 10]\n","    else:\n","        args.num_blocks = [5, 5, 5]\n","    \n","    if block_type == 'mlp':\n","        network = MyNetworkExample(64, block_type).to(device)\n","    else:\n","        network = MyNetwork(args.num_filters, block_type, args.num_blocks).to(device)\n","\n","    network.apply(weight_init)\n","    networks.append(network)\n","\n","# Count the number of parameters of the models. \n","# You can use it as an indicator of whether you correctly implemented the model.\n","\n","correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n","for network, block_type in zip(networks, block_types):\n","    # Print the number of parameters in each model.\n","    num_parameters = sum(p.numel() for p in network.parameters() if p.requires_grad)\n","    \n","    print(f'# of parameters in {block_type} net : {num_parameters}')\n","    print(f'Correct # of parameters in {block_type} net : {correct_params[block_type]}')"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Logs and ckpts will be saved in : /gdrive/My Drive/CS470/assignment1/results/trial_3\n","# of parameters in mlp net : 1649354\n","Correct # of parameters in mlp net : 1649354\n","# of parameters in conv net : 510426\n","Correct # of parameters in conv net : 510426\n","# of parameters in resPlain net : 510426\n","Correct # of parameters in resPlain net : 510426\n","# of parameters in resBottleneck net : 113946\n","Correct # of parameters in resBottleneck net : 113946\n","# of parameters in inception net : 124026\n","Correct # of parameters in inception net : 124026\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MKLm4c-WfwZ1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1600404393174,"user_tz":-540,"elapsed":13093741,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"29a83a13-2891-4742-c2f7-eb31880f93da"},"source":["final_accs = {}\n","\n","# Start training\n","for block_type, net in zip(block_types, networks):\n","    try:\n","        args.name = block_type\n","\n","        # Define optimizer\n","        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)\n","        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,80], gamma=0.5)\n","        \n","        # Create directories for logs and ckechpoints.\n","        ckpt_dir = parent_dir / args.name / args.ckpt_dir\n","        ckpt_dir.mkdir(parents=True, exist_ok=True)\n","        log_dir = parent_dir / args.name / args.log_dir\n","        log_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Create tensorboard writer,\n","        if args.tensorboard: \n","            writer = SummaryWriter(log_dir)\n","\n","        # Call the train & test function.\n","        t1 = time.time()\n","        accuracy = train_net(net, optimizer, scheduler, block_type, writer)\n","        t = time.time()-t1\n","        print(f'Best test accuracy of {block_type} network : {accuracy:.3f} took {t:.3f} secs')\n","        final_accs[f'{block_type}'] = accuracy*100\n","    except Exception as e:\n","        print(e)\n","\n","# Print final best accuracies of the models.\n","for key in final_accs.keys():\n","    print(f'Best accuracy of {key} = {final_accs[key]:.2f}%')\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Test result of epoch 0/100 || loss : 1.607 acc : 0.429 \n","Test result of epoch 1/100 || loss : 1.506 acc : 0.458 \n","Test result of epoch 2/100 || loss : 1.464 acc : 0.478 \n","Test result of epoch 3/100 || loss : 1.429 acc : 0.482 \n","Test result of epoch 4/100 || loss : 1.387 acc : 0.496 \n","Test result of epoch 5/100 || loss : 1.383 acc : 0.501 \n","Test result of epoch 6/100 || loss : 1.381 acc : 0.502 \n","Test result of epoch 7/100 || loss : 1.341 acc : 0.521 \n","Test result of epoch 8/100 || loss : 1.336 acc : 0.518 \n","Test result of epoch 9/100 || loss : 1.334 acc : 0.515 \n","Test result of epoch 10/100 || loss : 1.332 acc : 0.522 \n","Test result of epoch 11/100 || loss : 1.302 acc : 0.531 \n","Test result of epoch 12/100 || loss : 1.304 acc : 0.533 \n","Test result of epoch 13/100 || loss : 1.294 acc : 0.534 \n","Test result of epoch 14/100 || loss : 1.301 acc : 0.534 \n","Test result of epoch 15/100 || loss : 1.307 acc : 0.530 \n","Test result of epoch 16/100 || loss : 1.297 acc : 0.538 \n","Test result of epoch 17/100 || loss : 1.272 acc : 0.540 \n","Test result of epoch 18/100 || loss : 1.280 acc : 0.539 \n","Test result of epoch 19/100 || loss : 1.255 acc : 0.557 \n","Test result of epoch 20/100 || loss : 1.249 acc : 0.552 \n","Test result of epoch 21/100 || loss : 1.277 acc : 0.546 \n","Test result of epoch 22/100 || loss : 1.258 acc : 0.548 \n","Test result of epoch 23/100 || loss : 1.252 acc : 0.555 \n","Test result of epoch 24/100 || loss : 1.281 acc : 0.543 \n","Test result of epoch 25/100 || loss : 1.259 acc : 0.550 \n","Test result of epoch 26/100 || loss : 1.243 acc : 0.558 \n","Test result of epoch 27/100 || loss : 1.244 acc : 0.553 \n","Test result of epoch 28/100 || loss : 1.222 acc : 0.561 \n","Test result of epoch 29/100 || loss : 1.217 acc : 0.572 \n","Test result of epoch 30/100 || loss : 1.254 acc : 0.552 \n","Test result of epoch 31/100 || loss : 1.235 acc : 0.559 \n","Test result of epoch 32/100 || loss : 1.243 acc : 0.555 \n","Test result of epoch 33/100 || loss : 1.210 acc : 0.570 \n","Test result of epoch 34/100 || loss : 1.216 acc : 0.565 \n","Test result of epoch 35/100 || loss : 1.227 acc : 0.562 \n","Test result of epoch 36/100 || loss : 1.224 acc : 0.568 \n","Test result of epoch 37/100 || loss : 1.203 acc : 0.568 \n","Test result of epoch 38/100 || loss : 1.210 acc : 0.570 \n","Test result of epoch 39/100 || loss : 1.205 acc : 0.567 \n","Test result of epoch 40/100 || loss : 1.200 acc : 0.577 \n","Test result of epoch 41/100 || loss : 1.222 acc : 0.562 \n","Test result of epoch 42/100 || loss : 1.197 acc : 0.575 \n","Test result of epoch 43/100 || loss : 1.190 acc : 0.577 \n","Test result of epoch 44/100 || loss : 1.209 acc : 0.570 \n","Test result of epoch 45/100 || loss : 1.192 acc : 0.575 \n","Test result of epoch 46/100 || loss : 1.196 acc : 0.572 \n","Test result of epoch 47/100 || loss : 1.202 acc : 0.575 \n","Test result of epoch 48/100 || loss : 1.205 acc : 0.568 \n","Test result of epoch 49/100 || loss : 1.220 acc : 0.566 \n","Test result of epoch 50/100 || loss : 1.146 acc : 0.590 \n","Test result of epoch 51/100 || loss : 1.140 acc : 0.591 \n","Test result of epoch 52/100 || loss : 1.133 acc : 0.595 \n","Test result of epoch 53/100 || loss : 1.142 acc : 0.590 \n","Test result of epoch 54/100 || loss : 1.131 acc : 0.600 \n","Test result of epoch 55/100 || loss : 1.136 acc : 0.595 \n","Test result of epoch 56/100 || loss : 1.134 acc : 0.597 \n","Test result of epoch 57/100 || loss : 1.141 acc : 0.589 \n","Test result of epoch 58/100 || loss : 1.139 acc : 0.597 \n","Test result of epoch 59/100 || loss : 1.129 acc : 0.595 \n","Test result of epoch 60/100 || loss : 1.119 acc : 0.599 \n","Test result of epoch 61/100 || loss : 1.122 acc : 0.599 \n","Test result of epoch 62/100 || loss : 1.124 acc : 0.599 \n","Test result of epoch 63/100 || loss : 1.150 acc : 0.589 \n","Test result of epoch 64/100 || loss : 1.127 acc : 0.599 \n","Test result of epoch 65/100 || loss : 1.127 acc : 0.600 \n","Test result of epoch 66/100 || loss : 1.148 acc : 0.593 \n","Test result of epoch 67/100 || loss : 1.143 acc : 0.593 \n","Test result of epoch 68/100 || loss : 1.117 acc : 0.604 \n","Test result of epoch 69/100 || loss : 1.124 acc : 0.603 \n","Test result of epoch 70/100 || loss : 1.130 acc : 0.597 \n","Test result of epoch 71/100 || loss : 1.134 acc : 0.599 \n","Test result of epoch 72/100 || loss : 1.122 acc : 0.604 \n","Test result of epoch 73/100 || loss : 1.147 acc : 0.593 \n","Test result of epoch 74/100 || loss : 1.121 acc : 0.603 \n","Test result of epoch 75/100 || loss : 1.147 acc : 0.593 \n","Test result of epoch 76/100 || loss : 1.132 acc : 0.598 \n","Test result of epoch 77/100 || loss : 1.112 acc : 0.609 \n","Test result of epoch 78/100 || loss : 1.117 acc : 0.608 \n","Test result of epoch 79/100 || loss : 1.131 acc : 0.596 \n","Test result of epoch 80/100 || loss : 1.099 acc : 0.611 \n","Test result of epoch 81/100 || loss : 1.083 acc : 0.620 \n","Test result of epoch 82/100 || loss : 1.089 acc : 0.615 \n","Test result of epoch 83/100 || loss : 1.088 acc : 0.615 \n","Test result of epoch 84/100 || loss : 1.080 acc : 0.618 \n","Test result of epoch 85/100 || loss : 1.102 acc : 0.611 \n","Test result of epoch 86/100 || loss : 1.077 acc : 0.616 \n","Test result of epoch 87/100 || loss : 1.078 acc : 0.620 \n","Test result of epoch 88/100 || loss : 1.090 acc : 0.621 \n","Test result of epoch 89/100 || loss : 1.080 acc : 0.616 \n","Test result of epoch 90/100 || loss : 1.099 acc : 0.612 \n","Test result of epoch 91/100 || loss : 1.095 acc : 0.614 \n","Test result of epoch 92/100 || loss : 1.087 acc : 0.616 \n","Test result of epoch 93/100 || loss : 1.095 acc : 0.614 \n","Test result of epoch 94/100 || loss : 1.093 acc : 0.613 \n","Test result of epoch 95/100 || loss : 1.088 acc : 0.615 \n","Test result of epoch 96/100 || loss : 1.087 acc : 0.614 \n","Test result of epoch 97/100 || loss : 1.082 acc : 0.617 \n","Test result of epoch 98/100 || loss : 1.094 acc : 0.613 \n","Test result of epoch 99/100 || loss : 1.085 acc : 0.614 \n","Best test accuracy of mlp network : 0.621 took 1763.718 secs\n","Test result of epoch 0/100 || loss : 1.818 acc : 0.302 \n","Test result of epoch 1/100 || loss : 1.756 acc : 0.334 \n","Test result of epoch 2/100 || loss : 1.618 acc : 0.382 \n","Test result of epoch 3/100 || loss : 1.558 acc : 0.410 \n","Test result of epoch 4/100 || loss : 1.557 acc : 0.429 \n","Test result of epoch 5/100 || loss : 1.628 acc : 0.423 \n","Test result of epoch 6/100 || loss : 1.421 acc : 0.494 \n","Test result of epoch 7/100 || loss : 1.417 acc : 0.486 \n","Test result of epoch 8/100 || loss : 1.257 acc : 0.546 \n","Test result of epoch 9/100 || loss : 1.335 acc : 0.527 \n","Test result of epoch 10/100 || loss : 1.210 acc : 0.574 \n","Test result of epoch 11/100 || loss : 1.455 acc : 0.525 \n","Test result of epoch 12/100 || loss : 1.127 acc : 0.601 \n","Test result of epoch 13/100 || loss : 1.089 acc : 0.613 \n","Test result of epoch 14/100 || loss : 1.110 acc : 0.602 \n","Test result of epoch 15/100 || loss : 1.255 acc : 0.584 \n","Test result of epoch 16/100 || loss : 1.071 acc : 0.631 \n","Test result of epoch 17/100 || loss : 1.094 acc : 0.627 \n","Test result of epoch 18/100 || loss : 0.960 acc : 0.667 \n","Test result of epoch 19/100 || loss : 1.027 acc : 0.654 \n","Test result of epoch 20/100 || loss : 1.001 acc : 0.652 \n","Test result of epoch 21/100 || loss : 1.124 acc : 0.629 \n","Test result of epoch 22/100 || loss : 0.927 acc : 0.676 \n","Test result of epoch 23/100 || loss : 1.028 acc : 0.654 \n","Test result of epoch 24/100 || loss : 0.908 acc : 0.685 \n","Test result of epoch 25/100 || loss : 0.936 acc : 0.684 \n","Test result of epoch 26/100 || loss : 0.927 acc : 0.685 \n","Test result of epoch 27/100 || loss : 0.906 acc : 0.693 \n","Test result of epoch 28/100 || loss : 1.139 acc : 0.617 \n","Test result of epoch 29/100 || loss : 1.078 acc : 0.655 \n","Test result of epoch 30/100 || loss : 0.914 acc : 0.694 \n","Test result of epoch 31/100 || loss : 0.804 acc : 0.725 \n","Test result of epoch 32/100 || loss : 0.926 acc : 0.693 \n","Test result of epoch 33/100 || loss : 0.941 acc : 0.698 \n","Test result of epoch 34/100 || loss : 0.891 acc : 0.703 \n","Test result of epoch 35/100 || loss : 0.788 acc : 0.728 \n","Test result of epoch 36/100 || loss : 0.861 acc : 0.706 \n","Test result of epoch 37/100 || loss : 1.198 acc : 0.636 \n","Test result of epoch 38/100 || loss : 0.785 acc : 0.739 \n","Test result of epoch 39/100 || loss : 1.060 acc : 0.655 \n","Test result of epoch 40/100 || loss : 0.915 acc : 0.702 \n","Test result of epoch 41/100 || loss : 0.899 acc : 0.707 \n","Test result of epoch 42/100 || loss : 0.915 acc : 0.694 \n","Test result of epoch 43/100 || loss : 0.943 acc : 0.692 \n","Test result of epoch 44/100 || loss : 0.834 acc : 0.723 \n","Test result of epoch 45/100 || loss : 0.921 acc : 0.695 \n","Test result of epoch 46/100 || loss : 0.977 acc : 0.698 \n","Test result of epoch 47/100 || loss : 1.074 acc : 0.660 \n","Test result of epoch 48/100 || loss : 0.958 acc : 0.694 \n","Test result of epoch 49/100 || loss : 0.938 acc : 0.708 \n","Test result of epoch 50/100 || loss : 0.665 acc : 0.773 \n","Test result of epoch 51/100 || loss : 0.665 acc : 0.775 \n","Test result of epoch 52/100 || loss : 0.797 acc : 0.745 \n","Test result of epoch 53/100 || loss : 0.673 acc : 0.773 \n","Test result of epoch 54/100 || loss : 0.770 acc : 0.757 \n","Test result of epoch 55/100 || loss : 0.635 acc : 0.784 \n","Test result of epoch 56/100 || loss : 0.738 acc : 0.758 \n","Test result of epoch 57/100 || loss : 0.657 acc : 0.781 \n","Test result of epoch 58/100 || loss : 0.696 acc : 0.772 \n","Test result of epoch 59/100 || loss : 0.711 acc : 0.761 \n","Test result of epoch 60/100 || loss : 0.680 acc : 0.774 \n","Test result of epoch 61/100 || loss : 0.740 acc : 0.754 \n","Test result of epoch 62/100 || loss : 0.759 acc : 0.755 \n","Test result of epoch 63/100 || loss : 0.667 acc : 0.779 \n","Test result of epoch 64/100 || loss : 0.994 acc : 0.697 \n","Test result of epoch 65/100 || loss : 0.686 acc : 0.772 \n","Test result of epoch 66/100 || loss : 0.857 acc : 0.738 \n","Test result of epoch 67/100 || loss : 0.685 acc : 0.777 \n","Test result of epoch 68/100 || loss : 0.711 acc : 0.760 \n","Test result of epoch 69/100 || loss : 0.780 acc : 0.746 \n","Test result of epoch 70/100 || loss : 0.743 acc : 0.764 \n","Test result of epoch 71/100 || loss : 0.625 acc : 0.789 \n","Test result of epoch 72/100 || loss : 0.741 acc : 0.757 \n","Test result of epoch 73/100 || loss : 0.760 acc : 0.751 \n","Test result of epoch 74/100 || loss : 0.692 acc : 0.773 \n","Test result of epoch 75/100 || loss : 0.869 acc : 0.723 \n","Test result of epoch 76/100 || loss : 0.735 acc : 0.758 \n","Test result of epoch 77/100 || loss : 0.718 acc : 0.767 \n","Test result of epoch 78/100 || loss : 0.650 acc : 0.785 \n","Test result of epoch 79/100 || loss : 0.622 acc : 0.787 \n","Test result of epoch 80/100 || loss : 0.546 acc : 0.813 \n","Test result of epoch 81/100 || loss : 0.564 acc : 0.811 \n","Test result of epoch 82/100 || loss : 0.621 acc : 0.795 \n","Test result of epoch 83/100 || loss : 0.560 acc : 0.811 \n","Test result of epoch 84/100 || loss : 0.579 acc : 0.812 \n","Test result of epoch 85/100 || loss : 0.566 acc : 0.815 \n","Test result of epoch 86/100 || loss : 0.623 acc : 0.795 \n","Test result of epoch 87/100 || loss : 0.570 acc : 0.811 \n","Test result of epoch 88/100 || loss : 0.566 acc : 0.812 \n","Test result of epoch 89/100 || loss : 0.641 acc : 0.790 \n","Test result of epoch 90/100 || loss : 0.580 acc : 0.810 \n","Test result of epoch 91/100 || loss : 0.588 acc : 0.808 \n","Test result of epoch 92/100 || loss : 0.640 acc : 0.791 \n","Test result of epoch 93/100 || loss : 0.584 acc : 0.810 \n","Test result of epoch 94/100 || loss : 0.638 acc : 0.791 \n","Test result of epoch 95/100 || loss : 0.573 acc : 0.809 \n","Test result of epoch 96/100 || loss : 0.595 acc : 0.803 \n","Test result of epoch 97/100 || loss : 0.560 acc : 0.813 \n","Test result of epoch 98/100 || loss : 0.653 acc : 0.793 \n","Test result of epoch 99/100 || loss : 0.572 acc : 0.810 \n","Best test accuracy of conv network : 0.815 took 2399.383 secs\n","Test result of epoch 0/100 || loss : 1.790 acc : 0.345 \n","Test result of epoch 1/100 || loss : 1.680 acc : 0.383 \n","Test result of epoch 2/100 || loss : 1.435 acc : 0.476 \n","Test result of epoch 3/100 || loss : 1.331 acc : 0.514 \n","Test result of epoch 4/100 || loss : 1.176 acc : 0.577 \n","Test result of epoch 5/100 || loss : 1.222 acc : 0.580 \n","Test result of epoch 6/100 || loss : 1.186 acc : 0.602 \n","Test result of epoch 7/100 || loss : 0.928 acc : 0.681 \n","Test result of epoch 8/100 || loss : 0.869 acc : 0.703 \n","Test result of epoch 9/100 || loss : 0.930 acc : 0.685 \n","Test result of epoch 10/100 || loss : 0.773 acc : 0.729 \n","Test result of epoch 11/100 || loss : 0.731 acc : 0.741 \n","Test result of epoch 12/100 || loss : 0.724 acc : 0.752 \n","Test result of epoch 13/100 || loss : 0.707 acc : 0.754 \n","Test result of epoch 14/100 || loss : 0.779 acc : 0.737 \n","Test result of epoch 15/100 || loss : 0.713 acc : 0.764 \n","Test result of epoch 16/100 || loss : 0.644 acc : 0.782 \n","Test result of epoch 17/100 || loss : 0.598 acc : 0.796 \n","Test result of epoch 18/100 || loss : 0.677 acc : 0.774 \n","Test result of epoch 19/100 || loss : 0.586 acc : 0.801 \n","Test result of epoch 20/100 || loss : 0.622 acc : 0.797 \n","Test result of epoch 21/100 || loss : 0.581 acc : 0.806 \n","Test result of epoch 22/100 || loss : 0.594 acc : 0.801 \n","Test result of epoch 23/100 || loss : 0.597 acc : 0.801 \n","Test result of epoch 24/100 || loss : 0.597 acc : 0.802 \n","Test result of epoch 25/100 || loss : 0.639 acc : 0.792 \n","Test result of epoch 26/100 || loss : 0.572 acc : 0.809 \n","Test result of epoch 27/100 || loss : 0.540 acc : 0.816 \n","Test result of epoch 28/100 || loss : 0.494 acc : 0.832 \n","Test result of epoch 29/100 || loss : 0.552 acc : 0.813 \n","Test result of epoch 30/100 || loss : 0.535 acc : 0.821 \n","Test result of epoch 31/100 || loss : 0.712 acc : 0.771 \n","Test result of epoch 32/100 || loss : 0.562 acc : 0.816 \n","Test result of epoch 33/100 || loss : 0.513 acc : 0.830 \n","Test result of epoch 34/100 || loss : 0.502 acc : 0.831 \n","Test result of epoch 35/100 || loss : 0.569 acc : 0.816 \n","Test result of epoch 36/100 || loss : 0.528 acc : 0.827 \n","Test result of epoch 37/100 || loss : 0.551 acc : 0.822 \n","Test result of epoch 38/100 || loss : 0.610 acc : 0.808 \n","Test result of epoch 39/100 || loss : 0.560 acc : 0.818 \n","Test result of epoch 40/100 || loss : 0.658 acc : 0.792 \n","Test result of epoch 41/100 || loss : 0.520 acc : 0.833 \n","Test result of epoch 42/100 || loss : 0.521 acc : 0.823 \n","Test result of epoch 43/100 || loss : 0.572 acc : 0.814 \n","Test result of epoch 44/100 || loss : 0.535 acc : 0.824 \n","Test result of epoch 45/100 || loss : 0.525 acc : 0.825 \n","Test result of epoch 46/100 || loss : 0.504 acc : 0.836 \n","Test result of epoch 47/100 || loss : 0.506 acc : 0.834 \n","Test result of epoch 48/100 || loss : 0.549 acc : 0.825 \n","Test result of epoch 49/100 || loss : 0.532 acc : 0.826 \n","Test result of epoch 50/100 || loss : 0.402 acc : 0.869 \n","Test result of epoch 51/100 || loss : 0.435 acc : 0.862 \n","Test result of epoch 52/100 || loss : 0.439 acc : 0.862 \n","Test result of epoch 53/100 || loss : 0.426 acc : 0.866 \n","Test result of epoch 54/100 || loss : 0.423 acc : 0.864 \n","Test result of epoch 55/100 || loss : 0.431 acc : 0.859 \n","Test result of epoch 56/100 || loss : 0.456 acc : 0.855 \n","Test result of epoch 57/100 || loss : 0.441 acc : 0.860 \n","Test result of epoch 58/100 || loss : 0.440 acc : 0.859 \n","Test result of epoch 59/100 || loss : 0.480 acc : 0.853 \n","Test result of epoch 60/100 || loss : 0.469 acc : 0.860 \n","Test result of epoch 61/100 || loss : 0.447 acc : 0.860 \n","Test result of epoch 62/100 || loss : 0.453 acc : 0.859 \n","Test result of epoch 63/100 || loss : 0.437 acc : 0.866 \n","Test result of epoch 64/100 || loss : 0.426 acc : 0.866 \n","Test result of epoch 65/100 || loss : 0.496 acc : 0.849 \n","Test result of epoch 66/100 || loss : 0.451 acc : 0.863 \n","Test result of epoch 67/100 || loss : 0.438 acc : 0.864 \n","Test result of epoch 68/100 || loss : 0.443 acc : 0.867 \n","Test result of epoch 69/100 || loss : 0.459 acc : 0.859 \n","Test result of epoch 70/100 || loss : 0.450 acc : 0.862 \n","Test result of epoch 71/100 || loss : 0.506 acc : 0.850 \n","Test result of epoch 72/100 || loss : 0.435 acc : 0.866 \n","Test result of epoch 73/100 || loss : 0.472 acc : 0.846 \n","Test result of epoch 74/100 || loss : 0.431 acc : 0.863 \n","Test result of epoch 75/100 || loss : 0.480 acc : 0.853 \n","Test result of epoch 76/100 || loss : 0.472 acc : 0.859 \n","Test result of epoch 77/100 || loss : 0.553 acc : 0.838 \n","Test result of epoch 78/100 || loss : 0.457 acc : 0.864 \n","Test result of epoch 79/100 || loss : 0.474 acc : 0.856 \n","Test result of epoch 80/100 || loss : 0.398 acc : 0.881 \n","Test result of epoch 81/100 || loss : 0.405 acc : 0.882 \n","Test result of epoch 82/100 || loss : 0.430 acc : 0.872 \n","Test result of epoch 83/100 || loss : 0.405 acc : 0.885 \n","Test result of epoch 84/100 || loss : 0.434 acc : 0.878 \n","Test result of epoch 85/100 || loss : 0.428 acc : 0.880 \n","Test result of epoch 86/100 || loss : 0.430 acc : 0.882 \n","Test result of epoch 87/100 || loss : 0.474 acc : 0.870 \n","Test result of epoch 88/100 || loss : 0.441 acc : 0.877 \n","Test result of epoch 89/100 || loss : 0.441 acc : 0.881 \n","Test result of epoch 90/100 || loss : 0.447 acc : 0.879 \n","Test result of epoch 91/100 || loss : 0.472 acc : 0.867 \n","Test result of epoch 92/100 || loss : 0.433 acc : 0.874 \n","Test result of epoch 93/100 || loss : 0.461 acc : 0.875 \n","Test result of epoch 94/100 || loss : 0.460 acc : 0.875 \n","Test result of epoch 95/100 || loss : 0.439 acc : 0.879 \n","Test result of epoch 96/100 || loss : 0.444 acc : 0.883 \n","Test result of epoch 97/100 || loss : 0.460 acc : 0.875 \n","Test result of epoch 98/100 || loss : 0.447 acc : 0.878 \n","Test result of epoch 99/100 || loss : 0.465 acc : 0.870 \n","Best test accuracy of resPlain network : 0.885 took 2388.769 secs\n","Test result of epoch 0/100 || loss : 1.532 acc : 0.427 \n","Test result of epoch 1/100 || loss : 1.453 acc : 0.477 \n","Test result of epoch 2/100 || loss : 1.279 acc : 0.549 \n","Test result of epoch 3/100 || loss : 1.062 acc : 0.623 \n","Test result of epoch 4/100 || loss : 1.188 acc : 0.590 \n","Test result of epoch 5/100 || loss : 0.953 acc : 0.664 \n","Test result of epoch 6/100 || loss : 1.000 acc : 0.662 \n","Test result of epoch 7/100 || loss : 0.885 acc : 0.699 \n","Test result of epoch 8/100 || loss : 0.857 acc : 0.701 \n","Test result of epoch 9/100 || loss : 0.845 acc : 0.708 \n","Test result of epoch 10/100 || loss : 0.859 acc : 0.708 \n","Test result of epoch 11/100 || loss : 0.798 acc : 0.733 \n","Test result of epoch 12/100 || loss : 0.765 acc : 0.744 \n","Test result of epoch 13/100 || loss : 0.718 acc : 0.755 \n","Test result of epoch 14/100 || loss : 0.704 acc : 0.762 \n","Test result of epoch 15/100 || loss : 0.730 acc : 0.758 \n","Test result of epoch 16/100 || loss : 0.708 acc : 0.765 \n","Test result of epoch 17/100 || loss : 0.745 acc : 0.758 \n","Test result of epoch 18/100 || loss : 0.676 acc : 0.767 \n","Test result of epoch 19/100 || loss : 0.691 acc : 0.771 \n","Test result of epoch 20/100 || loss : 0.660 acc : 0.771 \n","Test result of epoch 21/100 || loss : 0.762 acc : 0.759 \n","Test result of epoch 22/100 || loss : 0.654 acc : 0.780 \n","Test result of epoch 23/100 || loss : 0.593 acc : 0.805 \n","Test result of epoch 24/100 || loss : 0.771 acc : 0.747 \n","Test result of epoch 25/100 || loss : 0.653 acc : 0.780 \n","Test result of epoch 26/100 || loss : 0.641 acc : 0.787 \n","Test result of epoch 27/100 || loss : 0.573 acc : 0.804 \n","Test result of epoch 28/100 || loss : 0.628 acc : 0.791 \n","Test result of epoch 29/100 || loss : 0.636 acc : 0.783 \n","Test result of epoch 30/100 || loss : 0.601 acc : 0.802 \n","Test result of epoch 31/100 || loss : 0.600 acc : 0.795 \n","Test result of epoch 32/100 || loss : 0.625 acc : 0.791 \n","Test result of epoch 33/100 || loss : 0.586 acc : 0.805 \n","Test result of epoch 34/100 || loss : 0.569 acc : 0.809 \n","Test result of epoch 35/100 || loss : 0.620 acc : 0.792 \n","Test result of epoch 36/100 || loss : 0.578 acc : 0.804 \n","Test result of epoch 37/100 || loss : 0.558 acc : 0.813 \n","Test result of epoch 38/100 || loss : 0.575 acc : 0.801 \n","Test result of epoch 39/100 || loss : 0.567 acc : 0.815 \n","Test result of epoch 40/100 || loss : 0.546 acc : 0.815 \n","Test result of epoch 41/100 || loss : 0.587 acc : 0.803 \n","Test result of epoch 42/100 || loss : 0.529 acc : 0.824 \n","Test result of epoch 43/100 || loss : 0.586 acc : 0.808 \n","Test result of epoch 44/100 || loss : 0.590 acc : 0.804 \n","Test result of epoch 45/100 || loss : 0.615 acc : 0.796 \n","Test result of epoch 46/100 || loss : 0.609 acc : 0.797 \n","Test result of epoch 47/100 || loss : 0.603 acc : 0.795 \n","Test result of epoch 48/100 || loss : 0.574 acc : 0.807 \n","Test result of epoch 49/100 || loss : 0.553 acc : 0.815 \n","Test result of epoch 50/100 || loss : 0.453 acc : 0.849 \n","Test result of epoch 51/100 || loss : 0.479 acc : 0.841 \n","Test result of epoch 52/100 || loss : 0.486 acc : 0.836 \n","Test result of epoch 53/100 || loss : 0.486 acc : 0.841 \n","Test result of epoch 54/100 || loss : 0.440 acc : 0.852 \n","Test result of epoch 55/100 || loss : 0.499 acc : 0.840 \n","Test result of epoch 56/100 || loss : 0.483 acc : 0.843 \n","Test result of epoch 57/100 || loss : 0.483 acc : 0.838 \n","Test result of epoch 58/100 || loss : 0.484 acc : 0.843 \n","Test result of epoch 59/100 || loss : 0.480 acc : 0.842 \n","Test result of epoch 60/100 || loss : 0.541 acc : 0.832 \n","Test result of epoch 61/100 || loss : 0.488 acc : 0.840 \n","Test result of epoch 62/100 || loss : 0.480 acc : 0.836 \n","Test result of epoch 63/100 || loss : 0.469 acc : 0.843 \n","Test result of epoch 64/100 || loss : 0.504 acc : 0.834 \n","Test result of epoch 65/100 || loss : 0.482 acc : 0.843 \n","Test result of epoch 66/100 || loss : 0.579 acc : 0.816 \n","Test result of epoch 67/100 || loss : 0.505 acc : 0.831 \n","Test result of epoch 68/100 || loss : 0.465 acc : 0.845 \n","Test result of epoch 69/100 || loss : 0.525 acc : 0.833 \n","Test result of epoch 70/100 || loss : 0.473 acc : 0.840 \n","Test result of epoch 71/100 || loss : 0.499 acc : 0.838 \n","Test result of epoch 72/100 || loss : 0.469 acc : 0.845 \n","Test result of epoch 73/100 || loss : 0.464 acc : 0.847 \n","Test result of epoch 74/100 || loss : 0.549 acc : 0.825 \n","Test result of epoch 75/100 || loss : 0.495 acc : 0.837 \n","Test result of epoch 76/100 || loss : 0.501 acc : 0.839 \n","Test result of epoch 77/100 || loss : 0.483 acc : 0.844 \n","Test result of epoch 78/100 || loss : 0.451 acc : 0.852 \n","Test result of epoch 79/100 || loss : 0.453 acc : 0.851 \n","Test result of epoch 80/100 || loss : 0.409 acc : 0.866 \n","Test result of epoch 81/100 || loss : 0.436 acc : 0.860 \n","Test result of epoch 82/100 || loss : 0.436 acc : 0.857 \n","Test result of epoch 83/100 || loss : 0.427 acc : 0.863 \n","Test result of epoch 84/100 || loss : 0.430 acc : 0.862 \n","Test result of epoch 85/100 || loss : 0.440 acc : 0.860 \n","Test result of epoch 86/100 || loss : 0.444 acc : 0.857 \n","Test result of epoch 87/100 || loss : 0.442 acc : 0.860 \n","Test result of epoch 88/100 || loss : 0.427 acc : 0.863 \n","Test result of epoch 89/100 || loss : 0.444 acc : 0.859 \n","Test result of epoch 90/100 || loss : 0.413 acc : 0.865 \n","Test result of epoch 91/100 || loss : 0.432 acc : 0.861 \n","Test result of epoch 92/100 || loss : 0.436 acc : 0.859 \n","Test result of epoch 93/100 || loss : 0.431 acc : 0.859 \n","Test result of epoch 94/100 || loss : 0.432 acc : 0.860 \n","Test result of epoch 95/100 || loss : 0.459 acc : 0.852 \n","Test result of epoch 96/100 || loss : 0.472 acc : 0.851 \n","Test result of epoch 97/100 || loss : 0.434 acc : 0.862 \n","Test result of epoch 98/100 || loss : 0.444 acc : 0.858 \n","Test result of epoch 99/100 || loss : 0.450 acc : 0.865 \n","Best test accuracy of resBottleneck network : 0.866 took 2754.679 secs\n","Test result of epoch 0/100 || loss : 1.760 acc : 0.323 \n","Test result of epoch 1/100 || loss : 1.688 acc : 0.378 \n","Test result of epoch 2/100 || loss : 1.530 acc : 0.435 \n","Test result of epoch 3/100 || loss : 1.436 acc : 0.483 \n","Test result of epoch 4/100 || loss : 1.354 acc : 0.514 \n","Test result of epoch 5/100 || loss : 1.281 acc : 0.549 \n","Test result of epoch 6/100 || loss : 1.219 acc : 0.560 \n","Test result of epoch 7/100 || loss : 1.216 acc : 0.567 \n","Test result of epoch 8/100 || loss : 1.106 acc : 0.601 \n","Test result of epoch 9/100 || loss : 1.197 acc : 0.602 \n","Test result of epoch 10/100 || loss : 1.060 acc : 0.616 \n","Test result of epoch 11/100 || loss : 1.018 acc : 0.648 \n","Test result of epoch 12/100 || loss : 1.098 acc : 0.622 \n","Test result of epoch 13/100 || loss : 1.048 acc : 0.643 \n","Test result of epoch 14/100 || loss : 1.039 acc : 0.635 \n","Test result of epoch 15/100 || loss : 0.923 acc : 0.673 \n","Test result of epoch 16/100 || loss : 1.009 acc : 0.654 \n","Test result of epoch 17/100 || loss : 0.938 acc : 0.681 \n","Test result of epoch 18/100 || loss : 1.136 acc : 0.623 \n","Test result of epoch 19/100 || loss : 1.054 acc : 0.647 \n","Test result of epoch 20/100 || loss : 0.929 acc : 0.679 \n","Test result of epoch 21/100 || loss : 0.932 acc : 0.671 \n","Test result of epoch 22/100 || loss : 0.893 acc : 0.686 \n","Test result of epoch 23/100 || loss : 0.879 acc : 0.697 \n","Test result of epoch 24/100 || loss : 0.750 acc : 0.735 \n","Test result of epoch 25/100 || loss : 0.803 acc : 0.721 \n","Test result of epoch 26/100 || loss : 0.835 acc : 0.716 \n","Test result of epoch 27/100 || loss : 0.821 acc : 0.717 \n","Test result of epoch 28/100 || loss : 0.778 acc : 0.726 \n","Test result of epoch 29/100 || loss : 0.817 acc : 0.715 \n","Test result of epoch 30/100 || loss : 0.896 acc : 0.694 \n","Test result of epoch 31/100 || loss : 0.819 acc : 0.720 \n","Test result of epoch 32/100 || loss : 0.831 acc : 0.721 \n","Test result of epoch 33/100 || loss : 0.788 acc : 0.732 \n","Test result of epoch 34/100 || loss : 0.838 acc : 0.714 \n","Test result of epoch 35/100 || loss : 0.714 acc : 0.749 \n","Test result of epoch 36/100 || loss : 0.806 acc : 0.724 \n","Test result of epoch 37/100 || loss : 0.834 acc : 0.718 \n","Test result of epoch 38/100 || loss : 0.787 acc : 0.728 \n","Test result of epoch 39/100 || loss : 0.711 acc : 0.755 \n","Test result of epoch 40/100 || loss : 0.695 acc : 0.759 \n","Test result of epoch 41/100 || loss : 0.703 acc : 0.758 \n","Test result of epoch 42/100 || loss : 0.664 acc : 0.774 \n","Test result of epoch 43/100 || loss : 0.698 acc : 0.761 \n","Test result of epoch 44/100 || loss : 1.044 acc : 0.664 \n","Test result of epoch 45/100 || loss : 0.769 acc : 0.744 \n","Test result of epoch 46/100 || loss : 0.807 acc : 0.730 \n","Test result of epoch 47/100 || loss : 0.693 acc : 0.759 \n","Test result of epoch 48/100 || loss : 0.664 acc : 0.768 \n","Test result of epoch 49/100 || loss : 0.638 acc : 0.780 \n","Test result of epoch 50/100 || loss : 0.567 acc : 0.805 \n","Test result of epoch 51/100 || loss : 0.588 acc : 0.798 \n","Test result of epoch 52/100 || loss : 0.617 acc : 0.795 \n","Test result of epoch 53/100 || loss : 0.568 acc : 0.804 \n","Test result of epoch 54/100 || loss : 0.643 acc : 0.783 \n","Test result of epoch 55/100 || loss : 0.619 acc : 0.786 \n","Test result of epoch 56/100 || loss : 0.548 acc : 0.814 \n","Test result of epoch 57/100 || loss : 0.599 acc : 0.797 \n","Test result of epoch 58/100 || loss : 0.609 acc : 0.792 \n","Test result of epoch 59/100 || loss : 0.563 acc : 0.808 \n","Test result of epoch 60/100 || loss : 0.585 acc : 0.800 \n","Test result of epoch 61/100 || loss : 0.594 acc : 0.795 \n","Test result of epoch 62/100 || loss : 0.707 acc : 0.768 \n","Test result of epoch 63/100 || loss : 0.572 acc : 0.802 \n","Test result of epoch 64/100 || loss : 0.681 acc : 0.777 \n","Test result of epoch 65/100 || loss : 0.590 acc : 0.796 \n","Test result of epoch 66/100 || loss : 0.575 acc : 0.805 \n","Test result of epoch 67/100 || loss : 0.622 acc : 0.795 \n","Test result of epoch 68/100 || loss : 0.564 acc : 0.811 \n","Test result of epoch 69/100 || loss : 0.615 acc : 0.794 \n","Test result of epoch 70/100 || loss : 0.553 acc : 0.813 \n","Test result of epoch 71/100 || loss : 0.636 acc : 0.786 \n","Test result of epoch 72/100 || loss : 0.602 acc : 0.792 \n","Test result of epoch 73/100 || loss : 0.566 acc : 0.807 \n","Test result of epoch 74/100 || loss : 0.596 acc : 0.800 \n","Test result of epoch 75/100 || loss : 0.559 acc : 0.810 \n","Test result of epoch 76/100 || loss : 0.631 acc : 0.788 \n","Test result of epoch 77/100 || loss : 0.591 acc : 0.799 \n","Test result of epoch 78/100 || loss : 0.584 acc : 0.806 \n","Test result of epoch 79/100 || loss : 0.600 acc : 0.795 \n","Test result of epoch 80/100 || loss : 0.488 acc : 0.832 \n","Test result of epoch 81/100 || loss : 0.493 acc : 0.834 \n","Test result of epoch 82/100 || loss : 0.487 acc : 0.834 \n","Test result of epoch 83/100 || loss : 0.508 acc : 0.834 \n","Test result of epoch 84/100 || loss : 0.531 acc : 0.822 \n","Test result of epoch 85/100 || loss : 0.532 acc : 0.823 \n","Test result of epoch 86/100 || loss : 0.588 acc : 0.810 \n","Test result of epoch 87/100 || loss : 0.500 acc : 0.834 \n","Test result of epoch 88/100 || loss : 0.510 acc : 0.829 \n","Test result of epoch 89/100 || loss : 0.493 acc : 0.836 \n","Test result of epoch 90/100 || loss : 0.513 acc : 0.829 \n","Test result of epoch 91/100 || loss : 0.501 acc : 0.832 \n","Test result of epoch 92/100 || loss : 0.509 acc : 0.829 \n","Test result of epoch 93/100 || loss : 0.497 acc : 0.831 \n","Test result of epoch 94/100 || loss : 0.496 acc : 0.830 \n","Test result of epoch 95/100 || loss : 0.545 acc : 0.819 \n","Test result of epoch 96/100 || loss : 0.526 acc : 0.825 \n","Test result of epoch 97/100 || loss : 0.520 acc : 0.830 \n","Test result of epoch 98/100 || loss : 0.493 acc : 0.836 \n","Test result of epoch 99/100 || loss : 0.511 acc : 0.827 \n","Best test accuracy of inception network : 0.836 took 3852.288 secs\n","Best accuracy of mlp = 62.06%\n","Best accuracy of conv = 81.46%\n","Best accuracy of resPlain = 88.48%\n","Best accuracy of resBottleneck = 86.55%\n","Best accuracy of inception = 83.64%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2Z1APMygJ8Um","colab_type":"text"},"source":["---\n","# 3.Aggregating experimental results and number of model parameters. (10pt)\n","\n","In this section, we automatically collect the classification performance of trained model. Also, we will count the number of parameters in the models. \n","You should match your own results with the values we provided. While the number of the parameters should be exactly same, classification accuarcy should be in the range of $\\pm$1.5% "]},{"cell_type":"code","metadata":{"id":"2MUJpr2SHYg_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600404431604,"user_tz":-540,"elapsed":12378,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}}},"source":["block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n","test_accs = {}\n","test_params= {}\n","\n","for block_type, net in zip(block_types, networks):\n","        ckpt_dir = parent_dir / block_type / args.ckpt_dir\n","        \n","        # load weights from best checkpoints.\n","        ckpt_path = f'{ckpt_dir}/{block_type}_best.pt'\n","        try:\n","            net.load_state_dict(torch.load(ckpt_path))\n","        except Exception as e:\n","            print(e)\n","\n","        # Measure test performance. \n","        with torch.no_grad():\n","            test_accuracy = 0.\n","            test_num_data = 0.\n","            for batch_idx, (x, y) in enumerate(test_dataloader):\n","                # Send `x` and `y` to either cpu or gpu using `device` variable..\n","                x = x.to(device=device)\n","                y = y.to(device=device)\n","\n","                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n","                logit = net(x)\n","\n","                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n","                loss = nn.CrossEntropyLoss()(logit, y)\n","\n","                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n","                accuracy = (logit.argmax(dim=1) == y).float().mean()\n","\n","                test_accuracy += accuracy.item()*x.shape[0]\n","                test_num_data += x.shape[0]\n","\n","            # Average classification accuracy.\n","            test_accuracy /= test_num_data\n","\n","            # Count the number of implemented models.\n","            num_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n","\n","            test_accs[f'{block_type}'] = test_accuracy*100\n","            test_params[f'{block_type}'] = num_parameters\n"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"cuQ_VHdpXbXz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1600404436443,"user_tz":-540,"elapsed":773,"user":{"displayName":"Keonwoo Kim","photoUrl":"","userId":"15131647585005276577"}},"outputId":"b904f1eb-d560-4b25-fede-867ad1090ce5"},"source":["# Printing final results.\n","correct_accs = {'mlp' : 62.6,'conv' : 81.9,'resPlain' : 88.6, 'resBottleneck' : 86.5, 'inception' : 83.7}\n","correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n","\n","print(' Method        | Accuracy   | # Params    | Expected Acc | Expected # Params  ')\n","print('------------------------------------------------------------------------------')\n","for block in block_types:\n","        print(f' {block:14}| {str(test_accs[block])[:5]:11}| {str(test_params[block]):11} | {str(correct_accs[block])[:5]:13}| {str(correct_params[block]):12}')\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":[" Method        | Accuracy   | # Params    | Expected Acc | Expected # Params  \n","------------------------------------------------------------------------------\n"," mlp           | 62.06      | 1649354     | 62.6         | 1649354     \n"," conv          | 81.46      | 510426      | 81.9         | 510426      \n"," resPlain      | 88.48      | 510426      | 88.6         | 510426      \n"," resBottleneck | 86.55      | 113946      | 86.5         | 113946      \n"," inception     | 83.64      | 124026      | 83.7         | 124026      \n"],"name":"stdout"}]}]}